{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import glob\n",
    "import dat\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "import re\n",
    "from io import StringIO\n",
    "import newlinejson as nlj\n",
    "from json import loads, dumps\n",
    "# import msgspec\n",
    "import gcsfs\n",
    "# import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crate_new_bucket(bucket_name, project_id):\n",
    "    # Instantiating storage class\n",
    "    storage_client = storage.Client(project_id)\n",
    "    # The name for the new bucket gathering from existing bucket\n",
    "    bucket_name_conv = f\"{bucket_name}_jsonl_conv\" # + \"jsonl_conv\"\n",
    "    # Creates the new bucket if bucket already exists using existing name with conversion\n",
    "    if storage_client.bucket(bucket_name_conv).exists():\n",
    "        bucket_new  = bucket_name_conv\n",
    "    # If bucket does not exists crates it and gets its name\n",
    "    else:\n",
    "        bucket_new = storage_client.create_bucket(bucket_name_conv, location='europe-west8')\n",
    "        bucket_new.location = 'europe-west8'\n",
    "        print(\"created bucket {} in {}\".format(bucket_new.name, bucket_new.location))\n",
    "        bucket_new  = bucket_new.name\n",
    "    # Returning created/existing bucket name as variable\n",
    "    return bucket_new\n",
    "\n",
    "def convert_json_jsonl(bucket_name,project_id):\n",
    "    # Instantiating needed classes and methods\n",
    "    storage_client = storage.Client(project_id)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "\n",
    "    # New bucket creation\n",
    "    new_bucket = crate_new_bucket(bucket_name,project_id)\n",
    "\n",
    "    # Loop through blobs in bucket and converting nljson with saving into new bucket path\n",
    "    for blob in blobs:\n",
    "        # Getting json file content as string\n",
    "        JSON_file = json.loads(blob.download_as_string(client=None))\n",
    "        # Converting to JSON to JSON New Line\n",
    "        nl_JSON_file = '\\n'.join([json.dumps(JSON_file)])\n",
    "        # That part is alternative if outer keys would like to be dispersed\n",
    "            # nl_JSON_file = '\\n'.join([json.dumps(JSON_file[outer_key], sort_keys=True) \n",
    "            #               for outer_key in sorted(JSON_file.keys(),\n",
    "            #                                       key=lambda x: int(x))])\n",
    "        # Calling new bucket name meta\n",
    "        bucket_new = storage_client.bucket(new_bucket)\n",
    "        # Blob name creation for converted blobs\n",
    "        blob_name = f'{blob.name}_converted_jsonl'\n",
    "        # Getting blob variable to apply read/write operations\n",
    "        blob = bucket_new.blob(blob_name)\n",
    "        # Writing new blob to new bucket\n",
    "        with blob.open(\"w\") as f:\n",
    "            f.write(nl_JSON_file)\n",
    "    return 'files_on_bucket_converted_toJSONnl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'files_on_bucket_converted_toJSONnl'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_json_jsonl(\"tryoutdavar\",'capable-memory-417812')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_bq(project_id, bucket_name):\n",
    "    # Instantiating big query client\n",
    "    bq_client = bigquery.Client(project_id)\n",
    "    # Getting bucket variable as source\n",
    "    bucket = storage.Client().bucket(bucket_name)\n",
    "    # If there are folders in bucket prefix part could be used\n",
    "    for blob in bucket.list_blobs(prefix=\"\"):\n",
    "        # Checking for json blobs as list_blobs also returns folder_name\n",
    "        if \".json\" in blob.name: \n",
    "            # Parsing JSON file name to create table on BQ with same file name\n",
    "            json_filename = blob.name.split('.json')[0]\n",
    "        #    json_filename = re.findall(r\".*/(.*).json\",blob.name) #Extracting file name for BQ's table id\n",
    "            # Determining table name\n",
    "            table_id = f\"capable-memory-417812.premiership.{json_filename}\" \n",
    "            try: # Check if table exists and operate TRUNCATE/WRITE ROWS\n",
    "               bq_client.get_table(table_id)\n",
    "               uri = f\"gs://{bucket_name}/{blob.name}\"\n",
    "               # Creating configs for load job operations (append)\n",
    "               job_config = bigquery.LoadJobConfig(\n",
    "               autodetect=True,\n",
    "               write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "               source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON)\n",
    "               print(\"Table {} already exists. Rows append operation be done.\".format(table_id))\n",
    "               load_job = bq_client.load_table_from_uri(\n",
    "                     uri, table_id, location = 'europe-west8',  \n",
    "                     job_config=job_config,\n",
    "               )  \n",
    "               # Make an API request.\n",
    "               load_job.result()  \n",
    "               # Waits for the job to complete.\n",
    "               destination_table = bq_client.get_table(table_id)  \n",
    "               # Make an API request.\n",
    "               print(\"Total {} rows exits on the table.\".format(destination_table.num_rows))\n",
    "           # If table is not found, upload it WRITE If table empty.  \n",
    "            except NotFound:   \n",
    "               uri = f\"gs://{bucket_name}/{blob.name}\"\n",
    "               print(uri)\n",
    "               job_config = bigquery.LoadJobConfig(\n",
    "               autodetect=True,\n",
    "               write_disposition=bigquery.WriteDisposition.WRITE_EMPTY,\n",
    "               source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON)\n",
    "               load_job = bq_client.load_table_from_uri(\n",
    "                     uri, table_id, location = 'europe-west8',  \n",
    "                     job_config=job_config,\n",
    "               )  \n",
    "               # Make an API request.\n",
    "               load_job.result()  \n",
    "               # Waits for the job to complete.\n",
    "               destination_table = bq_client.get_table(table_id)  # Make an API request.\n",
    "               print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table capable-memory-417812.premiership.epl_2022_2023_07_02_2023 already exists. Rows append operation be done.\n",
      "Total 1 rows exits on the table.\n",
      "Table capable-memory-417812.premiership.epl_2022_2023_07_02_2024 already exists. Rows append operation be done.\n",
      "Total 1 rows exits on the table.\n"
     ]
    }
   ],
   "source": [
    "write_to_bq('capable-memory-417812',\"tryoutdavar_jsonl_conv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json('gs://game_data_giray/epl_2022_2023_07_02_2023.json')\n",
    "# df = pd.read_json('gs://game_data_giray/premiership_short.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blob_elements(bucket_name):\n",
    "    \"\"\"Lists blob elements in mentioned bucket\"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "    # blob = bucket.blob(blob_name)\n",
    "    list_blobs = []\n",
    "   \n",
    "    for blob in blobs:\n",
    "        list_blobs.append(blob.name)\n",
    "        \n",
    "    return list_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['epl_2022_2023_07_02_2023.json_nl.json',\n",
       " 'epl_2022_2023_07_02_2024.json_nl.json']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_blob_elements(\"game_data_giray_jsonl_conv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crate_new_bucket(bucket_name, project_id):\n",
    "    # Instantiating storage class\n",
    "    storage_client = storage.Client(project_id)\n",
    "    # The name for the new bucket gathering from existing bucket\n",
    "    bucket_name_conv = f\"{bucket_name}_jsonl_conv\" # + \"jsonl_conv\"\n",
    "    print(bucket_name_conv)\n",
    "    # Creates the new bucket if bucket already exists using existing name with conversion\n",
    "    if storage_client.bucket(bucket_name_conv).exists():\n",
    "        print('eben')\n",
    "        bucket_new  = bucket_name_conv\n",
    "    # If bucket does not exists crates it and gets its name\n",
    "    else:\n",
    "        bucket_new = storage_client.create_bucket(bucket_name_conv, location='europe-west8')\n",
    "        bucket_new.location = 'europe-west8'\n",
    "        print(\"created bucket {} in {}\".format(bucket_new.name, bucket_new.location))\n",
    "        bucket_new  = bucket_new.name\n",
    "    return bucket_new\n",
    "\n",
    "\n",
    "def convert_json_jsonl(bucket_name, project_id, blob_name = [], path = []):\n",
    "    ''' This function reads multiple json files from location passed as parameter \n",
    "    and converts them to jsonl and saves into created bucket with an inner function\n",
    "    if bucket exists uses existing bucket'''\n",
    "    # New bucket creation\n",
    "    new_bucket = crate_new_bucket(bucket_name,project_id)\n",
    "\n",
    "    # Instantiating needed classes and methods\n",
    "    storage_client = storage.Client(project_id)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "    \n",
    "    # Loop through blobs in bucket and converting nljson with saving into new bucket path\n",
    "    for blob in blobs:\n",
    "        print(blob.name)\n",
    "        df = pd.read_json(\"gs://{}/{}\".format(bucket_name, blob.name)) \n",
    "        df.to_json(f'gs://{new_bucket}/{blob.name}',orient=\"records\",lines=True)\n",
    "    return 'json_files_converted_nljson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game_data_giray_jsonl_conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/1572037636.py:14: DeprecationWarning: Assignment to 'Bucket.location' is deprecated, as it is only valid before the bucket is created. Instead, pass the location to `Bucket.create`.\n",
      "  bucket_new.location = 'europe-west8'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created bucket game_data_giray_jsonl_conv in europe-west8\n",
      "epl_2022_2023_07_02_2023.json\n",
      "epl_2022_2023_07_02_2024.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'json_files_converted_nljson'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_json_jsonl(\"game_data_giray\",'capable-memory-417812')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
