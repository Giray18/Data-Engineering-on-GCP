{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import glob\n",
    "import dat\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "import re\n",
    "from io import StringIO\n",
    "import newlinejson as nlj\n",
    "from json import loads, dumps\n",
    "\n",
    "# import msgspec\n",
    "import gcsfs\n",
    "# import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'AZURE_STORAGE_ACCOUNT'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/vforvalbuena/GCP_AIRFLOW-4/playground.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://970-cs-ae99d6cf-1190-4062-981e-58e3cea72f43.cs-europe-west1-iuzs.cloudshell.dev/home/vforvalbuena/GCP_AIRFLOW-4/playground.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m os\u001b[39m.\u001b[39;49menviron[\u001b[39m'\u001b[39;49m\u001b[39mAZURE_STORAGE_ACCOUNT\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "File \u001b[0;32m/usr/lib/python3.9/os.py:679\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    676\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencodekey(key)]\n\u001b[1;32m    677\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    678\u001b[0m     \u001b[39m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'AZURE_STORAGE_ACCOUNT'"
     ]
    }
   ],
   "source": [
    "# os.environ['AZURE_STORAGE_ACCOUNT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crate_new_bucket(bucket_name, project_id):\n",
    "    # Instantiating storage class\n",
    "    storage_client = storage.Client(project_id)\n",
    "    # The name for the new bucket gathering from existing bucket\n",
    "    bucket_name_conv = f\"{bucket_name}_jsonl_conv\" # + \"jsonl_conv\"\n",
    "    # Creates the new bucket if bucket already exists using existing name with conversion\n",
    "    if storage_client.bucket(bucket_name_conv).exists():\n",
    "        bucket_new  = bucket_name_conv\n",
    "    # If bucket does not exists crates it and gets its name\n",
    "    else:\n",
    "        bucket_new = storage_client.create_bucket(bucket_name_conv, location='europe-west8')\n",
    "        bucket_new.location = 'europe-west8'\n",
    "        print(\"created bucket {} in {}\".format(bucket_new.name, bucket_new.location))\n",
    "        bucket_new  = bucket_new.name\n",
    "    # Returning created/existing bucket name as variable\n",
    "    return bucket_new\n",
    "\n",
    "def convert_json_jsonl(bucket_name,project_id):\n",
    "    # Instantiating needed classes and methods\n",
    "    storage_client = storage.Client(project_id)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "\n",
    "    # New bucket creation\n",
    "    new_bucket = crate_new_bucket(bucket_name,project_id)\n",
    "\n",
    "    # Loop through blobs in bucket and converting nljson with saving into new bucket path\n",
    "    for blob in blobs:\n",
    "        # Getting json file content as string\n",
    "        string_blob = blob.download_as_string(client=None)\n",
    "        # Append all file contents into one string to get one table combining all on BQ\n",
    "        string_comb = '\\n'.join(string_blob)\n",
    "    # Converting string to a dict JSON\n",
    "    JSON_file = json.loads(string_comb)\n",
    "    # Converting JSON to JSON New Line\n",
    "    # nl_JSON_file = '\\n'.join([json.dumps(JSON_file)])\n",
    "    # That part is alternative if outer keys would like to be dispersed\n",
    "    nl_JSON_file = '\\n'.join([json.dumps(JSON_file[outer_key], sort_keys=True) \n",
    "                        for outer_key in sorted(JSON_file.keys(),\n",
    "                                                key=lambda x: int(x))])\n",
    "    # Calling new bucket name meta\n",
    "    bucket_new = storage_client.bucket(new_bucket)\n",
    "    # Blob name creation for converted blobs\n",
    "    blob_name = f'{blob.name}_converted_jsonl'\n",
    "    # Getting blob variable to apply read/write operations\n",
    "    blob = bucket_new.blob(blob_name)\n",
    "    # Writing new blob to new bucket\n",
    "    with blob.open(\"w\") as f:\n",
    "        f.write(nl_JSON_file)\n",
    "    return 'files_on_bucket_converted_toJSONnl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-03-25T10:58:10.447+0000\u001b[0m] {\u001b[34m_default.py:\u001b[0m683} WARNING\u001b[0m - No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\u001b[0m\n",
      "[\u001b[34m2024-03-25T10:58:10.497+0000\u001b[0m] {\u001b[34m_default.py:\u001b[0m683} WARNING\u001b[0m - No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\u001b[0m\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, int found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/vforvalbuena/GCP_AIRFLOW-4/playground.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://970-cs-ae99d6cf-1190-4062-981e-58e3cea72f43.cs-europe-west1-iuzs.cloudshell.dev/home/vforvalbuena/GCP_AIRFLOW-4/playground.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m convert_json_jsonl(\u001b[39m\"\u001b[39;49m\u001b[39mtryoutdavar\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mcapable-memory-417812\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/home/vforvalbuena/GCP_AIRFLOW-4/playground.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://970-cs-ae99d6cf-1190-4062-981e-58e3cea72f43.cs-europe-west1-iuzs.cloudshell.dev/home/vforvalbuena/GCP_AIRFLOW-4/playground.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     string_blob \u001b[39m=\u001b[39m blob\u001b[39m.\u001b[39mdownload_as_string(client\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://970-cs-ae99d6cf-1190-4062-981e-58e3cea72f43.cs-europe-west1-iuzs.cloudshell.dev/home/vforvalbuena/GCP_AIRFLOW-4/playground.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m# Append all file contents into one string to get one table combining all on BQ\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://970-cs-ae99d6cf-1190-4062-981e-58e3cea72f43.cs-europe-west1-iuzs.cloudshell.dev/home/vforvalbuena/GCP_AIRFLOW-4/playground.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     string_comb \u001b[39m=\u001b[39m \u001b[39m'\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(string_blob)\n\u001b[1;32m     <a href='vscode-notebook-cell://970-cs-ae99d6cf-1190-4062-981e-58e3cea72f43.cs-europe-west1-iuzs.cloudshell.dev/home/vforvalbuena/GCP_AIRFLOW-4/playground.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Converting string to a dict JSON\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://970-cs-ae99d6cf-1190-4062-981e-58e3cea72f43.cs-europe-west1-iuzs.cloudshell.dev/home/vforvalbuena/GCP_AIRFLOW-4/playground.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m JSON_file \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(string_comb)\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, int found"
     ]
    }
   ],
   "source": [
    "convert_json_jsonl(\"tryoutdavar\",'capable-memory-417812')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crate_new_bucket(bucket_name, project_id):\n",
    "    # Instantiating storage class\n",
    "    storage_client = storage.Client(project_id)\n",
    "    # The name for the new bucket gathering from existing bucket\n",
    "    bucket_name_conv = f\"{bucket_name}_jsonl_conv\" # + \"jsonl_conv\"\n",
    "    # Creates the new bucket if bucket already exists using existing name with conversion\n",
    "    if storage_client.bucket(bucket_name_conv).exists():\n",
    "        bucket_new  = bucket_name_conv\n",
    "    # If bucket does not exists crates it and gets its name\n",
    "    else:\n",
    "        bucket_new = storage_client.create_bucket(bucket_name_conv, location='europe-west8')\n",
    "        bucket_new.location = 'europe-west8'\n",
    "        print(\"created bucket {} in {}\".format(bucket_new.name, bucket_new.location))\n",
    "        bucket_new  = bucket_new.name\n",
    "    # Returning created/existing bucket name as variable\n",
    "    return bucket_new\n",
    "\n",
    "def convert_json_jsonl(bucket_name,project_id):\n",
    "    # Instantiating needed classes and methods\n",
    "    storage_client = storage.Client(project_id)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "\n",
    "    # New bucket creation\n",
    "    new_bucket = crate_new_bucket(bucket_name,project_id)\n",
    "    # Combine JSONS\n",
    "    d_json = {}\n",
    "    global blob_name\n",
    "    blob_name = 'temp_blob'\n",
    "    # Loop through blobs in bucket and converting nljson with saving into new bucket path\n",
    "    for blob in blobs:\n",
    "        # Getting json file content as string and converting to dict\n",
    "        # JSON_file = json.loads(blob.download_as_string(client=None))\n",
    "        d_json.update(json.loads(blob.download_as_string(client=None)))\n",
    "        # Blob name creation for converted blobs\n",
    "        # blob_name = f'{blob.name}_converted_jsonl'\n",
    "        blob_name = blob_name = 'epl_2022_2023_season_stats.json'\n",
    "    # Converting to JSON to JSON New Line\n",
    "    # nl_JSON_file = '\\n'.join([json.dumps(d_json)])\n",
    "    # That part is alternative if outer keys would like to be dispersed\n",
    "    nl_JSON_file = '\\n'.join([json.dumps(d_json[outer_key], sort_keys=True) \n",
    "                        for outer_key in sorted(d_json.keys(),\n",
    "                                                key=lambda x: int(x))])\n",
    "    # Calling new bucket name meta\n",
    "    bucket_new = storage_client.bucket(new_bucket)\n",
    "    # Getting blob variable to apply read/write operations\n",
    "    blob = bucket_new.blob(blob_name)\n",
    "    # Writing new blob to new bucket\n",
    "    with blob.open(\"w\") as f:\n",
    "        f.write(nl_JSON_file)\n",
    "    return 'files_on_bucket_converted_toJSONnl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-03-25T12:36:16.823+0000\u001b[0m] {\u001b[34m_default.py:\u001b[0m683} WARNING\u001b[0m - No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\u001b[0m\n",
      "[\u001b[34m2024-03-25T12:36:16.838+0000\u001b[0m] {\u001b[34m_default.py:\u001b[0m683} WARNING\u001b[0m - No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'files_on_bucket_converted_toJSONnl'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_json_jsonl(\"tryoutdavar\",'capable-memory-417812')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_bq(project_id, bucket_name):\n",
    "    # Instantiating big query client\n",
    "    bq_client = bigquery.Client(project_id)\n",
    "    # Getting bucket variable as source\n",
    "    bucket = storage.Client().bucket(bucket_name)\n",
    "    # If there are folders in bucket prefix part could be used\n",
    "    for blob in bucket.list_blobs(prefix=\"\"):\n",
    "        # Checking for json blobs as list_blobs also returns folder_name\n",
    "        if \".json\" in blob.name: \n",
    "            # Parsing JSON file name to create table on BQ with same file name\n",
    "            json_filename = blob.name.split('.json')[0]\n",
    "        #    json_filename = re.findall(r\".*/(.*).json\",blob.name) #Extracting file name for BQ's table id\n",
    "            # Determining table name\n",
    "            table_id = f\"capable-memory-417812.premiership.{json_filename}\" \n",
    "            try: # Check if table exists and operate TRUNCATE/WRITE ROWS\n",
    "               bq_client.get_table(table_id)\n",
    "               uri = f\"gs://{bucket_name}/{blob.name}\"\n",
    "               # Creating configs for load job operations (append)\n",
    "               job_config = bigquery.LoadJobConfig(\n",
    "               autodetect=True,\n",
    "               write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "               source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON)\n",
    "               print(\"Table {} already exists. Rows append operation be done.\".format(table_id))\n",
    "               load_job = bq_client.load_table_from_uri(\n",
    "                     uri, table_id, location = 'europe-west8',  \n",
    "                     job_config=job_config,\n",
    "               )  \n",
    "               # Make an API request.\n",
    "               load_job.result()  \n",
    "               # Waits for the job to complete.\n",
    "               destination_table = bq_client.get_table(table_id)  \n",
    "               # Make an API request.\n",
    "               print(\"Total {} rows exits on the table.\".format(destination_table.num_rows))\n",
    "           # If table is not found, upload it WRITE If table empty.  \n",
    "            except NotFound:   \n",
    "               uri = f\"gs://{bucket_name}/{blob.name}\"\n",
    "               print(uri)\n",
    "               job_config = bigquery.LoadJobConfig(\n",
    "               autodetect=True,\n",
    "               write_disposition=bigquery.WriteDisposition.WRITE_EMPTY,\n",
    "               source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON)\n",
    "               load_job = bq_client.load_table_from_uri(\n",
    "                     uri, table_id, location = 'europe-west8',  \n",
    "                     job_config=job_config,\n",
    "               )  \n",
    "               # Make an API request.\n",
    "               load_job.result()  \n",
    "               # Waits for the job to complete.\n",
    "               destination_table = bq_client.get_table(table_id)  # Make an API request.\n",
    "               print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table capable-memory-417812.premiership.epl_2022_2023_07_02_2023 already exists. Rows append operation be done.\n",
      "Total 1 rows exits on the table.\n",
      "Table capable-memory-417812.premiership.epl_2022_2023_07_02_2024 already exists. Rows append operation be done.\n",
      "Total 1 rows exits on the table.\n"
     ]
    }
   ],
   "source": [
    "write_to_bq('capable-memory-417812',\"tryoutdavar_jsonl_conv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json('gs://game_data_giray/epl_2022_2023_07_02_2023.json')\n",
    "# df = pd.read_json('gs://game_data_giray/premiership_short.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blob_elements(bucket_name):\n",
    "    \"\"\"Lists blob elements in mentioned bucket\"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "    # blob = bucket.blob(blob_name)\n",
    "    list_blobs = []\n",
    "   \n",
    "    for blob in blobs:\n",
    "        list_blobs.append(blob.name)\n",
    "        \n",
    "    return list_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['epl_2022_2023_07_02_2023.json_nl.json',\n",
       " 'epl_2022_2023_07_02_2024.json_nl.json']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_blob_elements(\"game_data_giray_jsonl_conv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crate_new_bucket(bucket_name, project_id):\n",
    "    # Instantiating storage class\n",
    "    storage_client = storage.Client(project_id)\n",
    "    # The name for the new bucket gathering from existing bucket\n",
    "    bucket_name_conv = f\"{bucket_name}_jsonl_conv\" # + \"jsonl_conv\"\n",
    "    print(bucket_name_conv)\n",
    "    # Creates the new bucket if bucket already exists using existing name with conversion\n",
    "    if storage_client.bucket(bucket_name_conv).exists():\n",
    "        print('eben')\n",
    "        bucket_new  = bucket_name_conv\n",
    "    # If bucket does not exists crates it and gets its name\n",
    "    else:\n",
    "        bucket_new = storage_client.create_bucket(bucket_name_conv, location='europe-west8')\n",
    "        bucket_new.location = 'europe-west8'\n",
    "        print(\"created bucket {} in {}\".format(bucket_new.name, bucket_new.location))\n",
    "        bucket_new  = bucket_new.name\n",
    "    return bucket_new\n",
    "\n",
    "\n",
    "def convert_json_jsonl(bucket_name, project_id, blob_name = [], path = []):\n",
    "    ''' This function reads multiple json files from location passed as parameter \n",
    "    and converts them to jsonl and saves into created bucket with an inner function\n",
    "    if bucket exists uses existing bucket'''\n",
    "    # New bucket creation\n",
    "    new_bucket = crate_new_bucket(bucket_name,project_id)\n",
    "\n",
    "    # Instantiating needed classes and methods\n",
    "    storage_client = storage.Client(project_id)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "    \n",
    "    # Loop through blobs in bucket and converting nljson with saving into new bucket path\n",
    "    for blob in blobs:\n",
    "        print(blob.name)\n",
    "        df = pd.read_json(\"gs://{}/{}\".format(bucket_name, blob.name)) \n",
    "        df.to_json(f'gs://{new_bucket}/{blob.name}',orient=\"records\",lines=True)\n",
    "    return 'json_files_converted_nljson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game_data_giray_jsonl_conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/1572037636.py:14: DeprecationWarning: Assignment to 'Bucket.location' is deprecated, as it is only valid before the bucket is created. Instead, pass the location to `Bucket.create`.\n",
      "  bucket_new.location = 'europe-west8'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created bucket game_data_giray_jsonl_conv in europe-west8\n",
      "epl_2022_2023_07_02_2023.json\n",
      "epl_2022_2023_07_02_2024.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'json_files_converted_nljson'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_json_jsonl(\"game_data_giray\",'capable-memory-417812')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
