{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import glob\n",
    "import dat\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "import re\n",
    "from io import StringIO\n",
    "import newlinejson as nlj\n",
    "from json import loads, dumps\n",
    "# import msgspec\n",
    "import gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hello_gcs(project_id, bucket_name):\n",
    "#     bq_client = bigquery.Client(project_id)\n",
    "#     bucket = storage.Client().bucket(bucket_name)\n",
    "#     for blob in bucket.list_blobs(prefix=\"\"):\n",
    "#         if \".json\" in blob.name: #Checking for json blobs as list_blobs also returns folder_name\n",
    "#         #    print(blob.name)\n",
    "#            job_config = bigquery.LoadJobConfig(\n",
    "#             autodetect=True,\n",
    "#             source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON)\n",
    "#            json_filename = blob.name.split('.json')[0]\n",
    "#         #    json_filename = re.findall(r\".*/(.*).json\",blob.name) #Extracting file name for BQ's table id\n",
    "#         #    print(json_filename)\n",
    "#            table_id = f\"capable-memory-417812.premiership.{json_filename}\" # Determining table name\n",
    "#            print(table_id)\n",
    "#         #    try: #Check if the table already exists and skip uploading it.\n",
    "#         #        bq_client.get_table(table_id)\n",
    "#         #        print(\"Table {} already exists. Not uploaded.\".format(bq_table_id))\n",
    "#         #    except NotFound: #If table is not found, upload it.    \n",
    "#            uri = f\"gs://{bucket_name}/{blob.name}\"\n",
    "#            print(uri)\n",
    "#            load_job = bq_client.load_table_from_uri(\n",
    "#                uri, table_id, location = 'europe-west8',  \n",
    "#                job_config=job_config,\n",
    "#            )  # Make an API request.\n",
    "#            load_job.result()  # Waits for the job to complete.\n",
    "#            destination_table = bq_client.get_table(table_id)  # Make an API request.\n",
    "#            print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hello_gcs('capable-memory-417812',\"game_data_giray_jsonl_conv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json('gs://game_data_giray/epl_2022_2023_07_02_2023.json')\n",
    "# df = pd.read_json('gs://game_data_giray/premiership_short.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blob_elements(bucket_name):\n",
    "    \"\"\"Lists blob elements in mentioned bucket\"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "    # blob = bucket.blob(blob_name)\n",
    "    list_blobs = []\n",
    "   \n",
    "    for blob in blobs:\n",
    "        list_blobs.append(blob.name)\n",
    "        \n",
    "    return list_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['epl_2022_2023_07_02_2023.json_nl.json',\n",
       " 'epl_2022_2023_07_02_2024.json_nl.json']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_blob_elements(\"game_data_giray_jsonl_conv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crate_new_bucket(bucket_name, project_id):\n",
    "    # Instantiating storage class\n",
    "    storage_client = storage.Client(project_id)\n",
    "    # The name for the new bucket gathering from existing bucket\n",
    "    bucket_name_conv = f\"{bucket_name}_jsonl_conv\" # + \"jsonl_conv\"\n",
    "    print(bucket_name_conv)\n",
    "    # Creates the new bucket if bucket already exists using existing name with conversion\n",
    "    if storage_client.bucket(bucket_name_conv).exists():\n",
    "        print('eben')\n",
    "        bucket_new  = bucket_name_conv\n",
    "    # If bucket does not exists crates it and gets its name\n",
    "    else:\n",
    "        bucket_new = storage_client.create_bucket(bucket_name_conv, location='europe-west8')\n",
    "        bucket_new.location = 'europe-west8'\n",
    "        print(\"created bucket {} in {}\".format(bucket_new.name, bucket_new.location))\n",
    "        bucket_new  = bucket_new.name\n",
    "    return bucket_new\n",
    "\n",
    "\n",
    "def convert_json_jsonl(bucket_name, project_id, blob_name = [], path = []):\n",
    "    ''' This function reads multiple json files from location passed as parameter \n",
    "    and converts them to jsonl and saves into created bucket with an inner function\n",
    "    if bucket exists uses existing bucket'''\n",
    "    # New bucket creation\n",
    "    new_bucket = crate_new_bucket(bucket_name,project_id)\n",
    "\n",
    "    # Instantiating needed classes and methods\n",
    "    storage_client = storage.Client(project_id)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "    \n",
    "    # Loop through blobs in bucket and converting nljson with saving into new bucket path\n",
    "    for blob in blobs:\n",
    "        print(blob.name)\n",
    "        df = pd.read_json(\"gs://{}/{}\".format(bucket_name, blob.name)) \n",
    "        df.to_json(f'gs://{new_bucket}/{blob.name}',orient=\"records\",lines=True)\n",
    "    return 'json_files_converted_nljson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game_data_giray_jsonl_conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/1572037636.py:14: DeprecationWarning: Assignment to 'Bucket.location' is deprecated, as it is only valid before the bucket is created. Instead, pass the location to `Bucket.create`.\n",
      "  bucket_new.location = 'europe-west8'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created bucket game_data_giray_jsonl_conv in europe-west8\n",
      "epl_2022_2023_07_02_2023.json\n",
      "epl_2022_2023_07_02_2024.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'json_files_converted_nljson'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_json_jsonl(\"game_data_giray\",'capable-memory-417812')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
